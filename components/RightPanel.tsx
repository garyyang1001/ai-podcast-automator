import React, { useState, useCallback } from 'react';
import { Speaker, SeoMeta, ScriptMode, DialogLine } from '../types'; // Removed VoiceOption
import { AVAILABLE_VOICES, GEMINI_MODEL_TEXT } from '../constants';
import { Button, Select, TextInput } from './shared/FormControls';
import { ChevronDownIcon, ChevronUpIcon, Cog8ToothIcon, DocumentTextIcon, MusicalNoteIcon, UserIcon, UsersIcon, SparklesIcon, PlayCircleIcon, ClockIcon } from './icons/HeroIcons';
import { Spinner } from './shared/Spinner';
import JSZip from 'jszip';


interface RightPanelProps {
  scriptMode: ScriptMode;
  setScriptMode: (mode: ScriptMode) => void;
  speakers: Speaker[];
  updateSpeaker: (index: number, speaker: Speaker) => void;
  onGenerateScript: () => void;
  onGenerateSeoMeta: () => void;
  onDownloadScriptText: () => void; // Changed from onDownloadTranscript
  onDownloadTimedTranscriptSrt: () => void; // New handler for SRT
  seoMeta: SeoMeta | null;
  rssFeedUrl: string;
  setRssFeedUrl: (url: string) => void;
  isLoading: boolean; 
  isSynthesizingAudio: boolean; 
  setIsSynthesizingAudio: (loading: boolean) => void; 
  setError: (message: string | null) => void;
  dialogLines: DialogLine[];
  actualAudioDurations: Record<string, number>; // Added to manage SRT button state
  panelHeightClass?: string; 
  onAudioSegmentSynthesized: (lineId: string, duration: number) => void;
}

const AccordionSection: React.FC<{ title: string, icon?: React.ReactNode, children: React.ReactNode, defaultOpen?: boolean, id?: string }> = ({ title, icon, children, defaultOpen = false, id }) => {
  const [isOpen, setIsOpen] = useState(defaultOpen);
  const sectionId = id || `accordion-${title.replace(/\s+/g, '-').toLowerCase()}`;
  return (
    <div className="border border-slate-700 rounded-lg">
      <button
        onClick={() => setIsOpen(!isOpen)}
        className="w-full flex justify-between items-center p-3 bg-slate-800 hover:bg-slate-750 transition-colors rounded-t-lg"
        aria-expanded={isOpen}
        aria-controls={sectionId}
      >
        <div className="flex items-center">
          {icon && <span className="mr-2">{icon}</span>}
          <span className="font-semibold text-slate-200">{title}</span>
        </div>
        {isOpen ? <ChevronUpIcon className="w-5 h-5 text-slate-400" /> : <ChevronDownIcon className="w-5 h-5 text-slate-400" />}
      </button>
      {isOpen && <div id={sectionId} className="p-4 bg-slate-850 rounded-b-lg space-y-3">{children}</div>}
    </div>
  );
};

// Helper function to get audio duration from a Blob
const getAudioBlobDuration = (blob: Blob): Promise<number> => {
  return new Promise((resolve, reject) => {
    const audio = document.createElement('audio');
    const objectUrl = URL.createObjectURL(blob);
    audio.src = objectUrl;
    let timeoutId: number; // Declare timeoutId
    
    const cleanup = () => {
      clearTimeout(timeoutId); // Clear timeout
      URL.revokeObjectURL(objectUrl);
      audio.removeEventListener('loadedmetadata', onLoadedMetadata);
      audio.removeEventListener('error', onError);
    };

    const onLoadedMetadata = () => {
      cleanup();
      resolve(audio.duration);
    };

    const onError = (e: Event | string) => {
      cleanup();
      console.error("Error loading audio for duration measurement:", e);
      reject(new Error("Could not measure audio duration."));
    };

    audio.addEventListener('loadedmetadata', onLoadedMetadata);
    audio.addEventListener('error', onError);
    
    timeoutId = window.setTimeout(() => { // Use window.setTimeout for clarity in browser
        onError("Timeout while waiting for audio metadata.");
    }, 10000); 

    audio.load(); 
  });
};


export const RightPanel: React.FC<RightPanelProps> = ({
  scriptMode,
  setScriptMode,
  speakers,
  updateSpeaker,
  onGenerateScript,
  onGenerateSeoMeta,
  onDownloadScriptText,
  onDownloadTimedTranscriptSrt,
  seoMeta,
  rssFeedUrl,
  setRssFeedUrl,
  isLoading,
  isSynthesizingAudio,
  setIsSynthesizingAudio,
  setError,
  dialogLines,
  actualAudioDurations,
  panelHeightClass,
  onAudioSegmentSynthesized,
}) => {
  const handleSpeakerChange = (index: number, field: keyof Omit<Speaker, 'id'|'color'|'dotColor'>, value: string) => {
    updateSpeaker(index, { ...speakers[index], [field]: value });
  };

  // ğŸ”¥ ä¿®æ­£çš„ TTS API å‡½æ•¸ - ä½¿ç”¨ Google Cloud TTS API æ”¯æ´ Chirp 3 HD
  const synthesizeSpeechInternal = useCallback(async (text: string, voiceId: string, languageCode: string = 'cmn-TW'): Promise<string | null> => {
    const apiKey = process.env.GOOGLE_CLOUD_TTS_API_KEY;

    if (!apiKey) {
      setError("Google Cloud API Key (GOOGLE_CLOUD_TTS_API_KEY) not found. Please configure it in .env.");
      return null;
    }
    
    // ä½¿ç”¨ Google Cloud Text-to-Speech APIï¼Œæ”¯æ´ Chirp 3 HD
    const ttsApiEndpoint = `https://texttospeech.googleapis.com/v1/text:synthesize?key=${apiKey}`;

    try {
      const response = await fetch(ttsApiEndpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          input: { text: text },
          voice: { 
            languageCode: languageCode, 
            name: voiceId 
          },
          audioConfig: { audioEncoding: "MP3" }
        }),
      });

      if (!response.ok) {
        const errorData = await response.json();
        console.error("Google Cloud TTS API Error:", errorData);
        throw new Error(errorData.error?.message || `Google Cloud TTS API request failed: ${response.statusText}`);
      }

      const result = await response.json();
      if (result.audioContent) {
        return result.audioContent; 
      } else {
        console.error("Google Cloud TTS API did not return expected audio content structure:", result);
        throw new Error("Google Cloud TTS API did not return audio content in the expected format.");
      }
    } catch (e) {
      console.error("Error synthesizing speech via Google Cloud TTS:", e);
      throw e; 
    }
  }, [setError]);


  const handlePreviewVoice = useCallback(async (speakerIndex: number) => {
    const speaker = speakers[speakerIndex];
    if (!speaker || isSynthesizingAudio) return;

    setIsSynthesizingAudio(true);
    setError(null);

    const voiceOption = AVAILABLE_VOICES.find(v => v.id === speaker.voice);
    const voiceStyleName = voiceOption ? voiceOption.name : "é è¨­é¢¨æ ¼";
    const speakerName = speaker.name || `ç™¼è¨€äºº ${speakerIndex + 1}`;
    const textToSpeak = `é€™æ˜¯ ${speakerName} ä½¿ç”¨ ${voiceStyleName} é¢¨æ ¼è¨­å®šçš„èªéŸ³é è¦½ã€‚ä½ å¥½å—ï¼Ÿé€™æ˜¯ç”±Google Cloud Text-to-Speechç”¢ç”Ÿã€‚`;
    
    try {
        const audioContentBase64 = await synthesizeSpeechInternal(textToSpeak, speaker.voice);
        if (audioContentBase64) {
            const audioSrc = `data:audio/mp3;base64,${audioContentBase64}`;
            const audio = new Audio(audioSrc);
            audio.play().catch(e => {
                console.error("Error playing audio:", e);
                setError("ç„¡æ³•æ’­æ”¾é è¦½èªéŸ³ï¼Œç€è¦½å™¨å¯èƒ½é™åˆ¶äº†è‡ªå‹•æ’­æ”¾ã€‚");
            });
        }
    } catch (e) {
        setError(e instanceof Error ? e.message : "An unknown error occurred during voice preview synthesis.");
    } finally {
        setIsSynthesizingAudio(false);
    }
  }, [speakers, synthesizeSpeechInternal, setError, isSynthesizingAudio, setIsSynthesizingAudio]);

  const handleGenerateFullPodcastAudio = useCallback(async () => {
    if (dialogLines.length === 0) {
      setError("è«‹å…ˆç”Ÿæˆè…³æœ¬æ‰èƒ½ç”¢ç”ŸèªéŸ³ã€‚");
      return;
    }
    if (isSynthesizingAudio) return;

    setIsSynthesizingAudio(true);
    setError(null);
    
    const audioSegments: { name: string; data: Blob }[] = [];
    let hasErrorOccurred = false;

    for (let i = 0; i < dialogLines.length; i++) {
      const line = dialogLines[i];
      const speaker = speakers.find(s => s.id === line.speakerId);

      if (!speaker) {
        setError(`ç¬¬ ${i + 1} è¡Œå°è©±æ‰¾ä¸åˆ°ç™¼è¨€äººè¨­å®šã€‚`);
        hasErrorOccurred = true;
        break; 
      }
      
      try {
        const audioContentBase64 = await synthesizeSpeechInternal(line.text, speaker.voice);

        if (audioContentBase64) {
          const byteCharacters = atob(audioContentBase64);
          const byteNumbers = new Array(byteCharacters.length);
          for (let j = 0; j < byteCharacters.length; j++) {
            byteNumbers[j] = byteCharacters.charCodeAt(j);
          }
          const byteArray = new Uint8Array(byteNumbers);
          const blob = new Blob([byteArray], { type: 'audio/mp3' });
          
          try {
            const duration = await getAudioBlobDuration(blob);
            onAudioSegmentSynthesized(line.id, duration);
          } catch (durationError) {
            console.warn(`Could not get duration for line ${i + 1}: ${line.text}. Using estimated duration for transcript if needed.`, durationError);
            onAudioSegmentSynthesized(line.id, -1); // Indicate failure 
          }

          const safeSpeakerName = speaker.name.replace(/[^\w\s\u4e00-\u9fa5]/gi, '').replace(/\s+/g, '_'); 
          const fileName = `podcast_segment_${String(i + 1).padStart(2, '0')}_${safeSpeakerName}.mp3`;
          audioSegments.push({ name: fileName, data: blob });
        } else {
           setError(`ç¬¬ ${i + 1} è¡ŒèªéŸ³åˆæˆå¤±æ•—ï¼Œæœªæ”¶åˆ°éŸ³è¨Šå…§å®¹ã€‚`);
           hasErrorOccurred = true;
           break;
        }
      } catch (e) {
        console.error(`Error synthesizing audio for line ${i + 1}:`, e);
        setError(`ç¬¬ ${i + 1} è¡ŒèªéŸ³åˆæˆå¤±æ•—: ${e instanceof Error ? e.message : "æœªçŸ¥éŒ¯èª¤"}`);
        hasErrorOccurred = true;
        break; 
      }
    } 

    if (!hasErrorOccurred && audioSegments.length > 0) {
      try {
        const zip = new JSZip();
        audioSegments.forEach(segment => {
          zip.file(segment.name, segment.data);
        });
        
        const zipBlob = await zip.generateAsync({ type: "blob" });
        const link = document.createElement('a');
        link.href = URL.createObjectURL(zipBlob);
        link.download = 'podcast_audio_segments.zip';
        document.body.appendChild(link);
        link.click();
        document.body.removeChild(link);
        URL.revokeObjectURL(link.href);
      } catch (e) {
        console.error("Error creating or downloading ZIP file:", e);
        setError(`ç”Ÿæˆ ZIP å£“ç¸®æª”å¤±æ•—: ${e instanceof Error ? e.message : "æœªçŸ¥éŒ¯èª¤"}`);
      }
    }

    setIsSynthesizingAudio(false);

  }, [dialogLines, speakers, synthesizeSpeechInternal, setError, isSynthesizingAudio, setIsSynthesizingAudio, onAudioSegmentSynthesized]);

  const canDownloadTimedTranscript = dialogLines.length > 0 && 
                                   Object.keys(actualAudioDurations).length === dialogLines.length &&
                                   dialogLines.every(line => actualAudioDurations[line.id] !== undefined && actualAudioDurations[line.id] >= 0);


  return (
    <div 
      className={`lg:w-1/3 bg-slate-900 p-6 rounded-lg shadow-xl space-y-6 overflow-y-auto custom-scrollbar 
                 lg:sticky lg:top-4 ${panelHeightClass || ''}`}
    >
      <h2 className="text-lg font-semibold text-emerald-400 mb-1">4. åŸ·è¡Œèˆ‡è¨­å®š (Run & Settings)</h2>
      
      <div className="space-y-3">
        <label htmlFor="ai-model-display" className="block text-sm font-medium text-slate-300">AI æ–‡å­—æ¨¡å‹ (AI Text Model)</label>
        <div id="ai-model-display" className="bg-slate-800 p-3 rounded-md text-sm border border-slate-700">{GEMINI_MODEL_TEXT} (Text Generation)</div>
      
        <fieldset className="space-y-1">
            <legend className="block text-sm font-medium text-slate-300 mb-1">æ¨¡å¼ (Mode)</legend>
            <div className="grid grid-cols-2 gap-2">
              <Button onClick={() => setScriptMode(ScriptMode.SINGLE)} variant={scriptMode === ScriptMode.SINGLE ? 'primary' : 'secondary'} fullWidth aria-pressed={scriptMode === ScriptMode.SINGLE}>
                <UserIcon className="w-5 h-5 mr-2"/> å–®äººä¸»æŒ
              </Button>
              <Button onClick={() => setScriptMode(ScriptMode.MULTI)} variant={scriptMode === ScriptMode.MULTI ? 'primary' : 'secondary'} fullWidth aria-pressed={scriptMode === ScriptMode.MULTI}>
                <UsersIcon className="w-5 h-5 mr-2"/> å¤šäººå°è©±
              </Button>
            </div>
        </fieldset>
      </div>

      <AccordionSection title="èªéŸ³è¨­å®š (Voice Settings)" icon={<Cog8ToothIcon className="w-5 h-5 text-slate-400" />} defaultOpen={true} id="voice-settings-section">
        {(scriptMode === ScriptMode.SINGLE ? [speakers[0]] : speakers).map((speaker, originalIndex) => {
          const speakerArrayIndex = scriptMode === ScriptMode.SINGLE ? speakers.findIndex(s => s.id === speaker.id) : originalIndex;
          if (!speaker) return null; 

          return (
          <div key={speaker.id} className="p-3 border border-slate-700 rounded-md bg-slate-800">
            <div className="flex items-center mb-2">
              <span className={`w-3 h-3 rounded-full ${speaker.dotColor} mr-2`}></span>
              <h4 className="font-semibold text-sm text-slate-300">ç™¼è¨€äºº {originalIndex + 1} è¨­å®š</h4>
            </div>
            <TextInput
              label="åç¨± (Name)"
              id={`speaker-name-${speaker.id}`}
              value={speaker.name}
              onChange={(e) => handleSpeakerChange(speakerArrayIndex, 'name', e.target.value)}
              placeholder={`ä¾‹å¦‚ï¼šä¸»æŒäºº ${originalIndex + 1}`}
            />
            <div className="flex items-end space-x-2">
                <Select
                    label="Google Cloud TTS èªéŸ³ (æ”¯æ´ Chirp 3 HD)"
                    id={`speaker-voice-style-${speaker.id}`}
                    value={speaker.voice} 
                    onChange={(e) => handleSpeakerChange(speakerArrayIndex, 'voice', e.target.value)}
                    options={AVAILABLE_VOICES.map(v => ({ value: v.id, label: v.name }))}
                    className="flex-grow"
                    helperText="é¸æ“‡ Google Cloud TTS èªéŸ³ï¼ŒåŒ…å«æœ€æ–°çš„ Chirp 3 HD é«˜å“è³ªèªéŸ³ã€‚"
                />
                <Button 
                    onClick={() => handlePreviewVoice(speakerArrayIndex)} 
                    variant="outline" 
                    size="md" 
                    className="mb-3 flex-shrink-0"
                    aria-label={`é è¦½ ${speaker.name} çš„ Google Cloud TTS èªéŸ³`}
                    disabled={isSynthesizingAudio}
                >
                    {isSynthesizingAudio && 
                     speakers[speakerArrayIndex] === speaker && <Spinner/>} 
                    {(!isSynthesizingAudio || speakers[speakerArrayIndex] !== speaker) && <PlayCircleIcon className="w-5 h-5"/>}
                </Button>
            </div>
          </div>
        )}))}
      </AccordionSection>
      
      <div className="space-y-3 pt-3">
        <Button onClick={onGenerateScript} disabled={isLoading || isSynthesizingAudio} fullWidth variant="primary" aria-label="Generate podcast script">
          {(isLoading && !isSynthesizingAudio) ? <Spinner /> : <SparklesIcon className="w-5 h-5 mr-2" />} ç”Ÿæˆè…³æœ¬
        </Button>
         <Button onClick={onGenerateSeoMeta} disabled={isLoading || isSynthesizingAudio || dialogLines.length === 0} fullWidth variant="secondary" aria-label="Generate SEO metadata">
           {(isLoading && !isSynthesizingAudio) ? <Spinner /> : <SparklesIcon className="w-5 h-5 mr-2" />} ç”Ÿæˆ SEO Meta
        </Button>
      </div>

      <AccordionSection title="ç”¢å‡ºèˆ‡ç™¼ä½ˆ (Output & Publishing)" icon={<DocumentTextIcon className="w-5 h-5 text-slate-400" />} id="output-publishing-section">
        {seoMeta && (
          <div className="bg-slate-800 p-3 rounded-md border border-slate-700">
            <h4 className="font-semibold text-sm text-sky-400 mb-1">SEO Meta:</h4>
            <p className="text-xs text-slate-300"><strong>Title:</strong> {seoMeta.title}</p>
            <p className="text-xs text-slate-300"><strong>Description:</strong> {seoMeta.description}</p>
          </div>
        )}
        <Button onClick={onDownloadScriptText} fullWidth variant="outline" aria-label="ä¸‹è¼‰è…³æœ¬æ–‡å­— (TXT)" disabled={dialogLines.length === 0 || isSynthesizingAudio}>
          <DocumentTextIcon className="w-5 h-5 mr-2"/> ä¸‹è¼‰è…³æœ¬æ–‡å­— (TXT)
        </Button>
        <Button 
          onClick={onDownloadTimedTranscriptSrt} 
          fullWidth 
          variant="outline" 
          aria-label="ä¸‹è¼‰æ™‚é–“ç¢¼é€å­—ç¨¿ (SRT)" 
          disabled={!canDownloadTimedTranscript || isSynthesizingAudio}
          title={!canDownloadTimedTranscript ? "è«‹å…ˆã€Œç”¢ç”Ÿ AI Podcast èªéŸ³ã€ä»¥ç²å–ç²¾ç¢ºæ™‚é–“æˆ³ã€‚" : "ä¸‹è¼‰ SRT æ ¼å¼çš„æ™‚é–“ç¢¼é€å­—ç¨¿"}
        >
          <ClockIcon className="w-5 h-5 mr-2"/> ä¸‹è¼‰æ™‚é–“ç¢¼é€å­—ç¨¿ (SRT)
        </Button>
        <Button 
          onClick={handleGenerateFullPodcastAudio} 
          fullWidth 
          variant="outline" 
          aria-label="ç”¢ç”Ÿ AI Podcast èªéŸ³" 
          disabled={dialogLines.length === 0 || isSynthesizingAudio}
        >
         {isSynthesizingAudio ? <Spinner/> : <MusicalNoteIcon className="w-5 h-5 mr-2"/>} 
         ç”¢ç”Ÿ AI Podcast èªéŸ³
        </Button>
        <TextInput
          label="RSS Feed ç™¼ä½ˆç¶²å€ (RSS Feed URL - for reference)"
          id="rss-feed-url"
          value={rssFeedUrl}
          onChange={(e) => setRssFeedUrl(e.target.value)}
          placeholder="https://anchor.fm/s/your-id/podcast/rss"
          helperText="æ­¤æ¬„ä½åƒ…ä¾›è¨˜éŒ„ï¼Œç›®å‰ä¸æ”¯æ´è‡ªå‹•ä¸Šå‚³ã€‚"
        />
      </AccordionSection>
    </div>
  );
};
