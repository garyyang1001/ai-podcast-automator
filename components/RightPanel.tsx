import React, { useState, useCallback } from 'react';
import { Speaker, SeoMeta, ScriptMode, DialogLine } from '../types';
import { AVAILABLE_VOICES, GEMINI_MODEL_TEXT } from '../constants';
import { Button, Select, TextInput } from './shared/FormControls';
import { ChevronDownIcon, ChevronUpIcon, Cog8ToothIcon, DocumentTextIcon, MusicalNoteIcon, UserIcon, UsersIcon, SparklesIcon, PlayCircleIcon, ClockIcon } from './icons/HeroIcons';
import { Spinner } from './shared/Spinner';
import JSZip from 'jszip';

interface RightPanelProps {
  scriptMode: ScriptMode;
  setScriptMode: (mode: ScriptMode) => void;
  speakers: Speaker[];
  updateSpeaker: (index: number, speaker: Speaker) => void;
  onGenerateScript: () => void;
  onGenerateSeoMeta: () => void;
  onDownloadScriptText: () => void;
  onDownloadTimedTranscriptSrt: () => void;
  seoMeta: SeoMeta | null;
  rssFeedUrl: string;
  setRssFeedUrl: (url: string) => void;
  isLoading: boolean; 
  isSynthesizingAudio: boolean; 
  setIsSynthesizingAudio: (loading: boolean) => void; 
  setError: (message: string | null) => void;
  dialogLines: DialogLine[];
  actualAudioDurations: Record<string, number>;
  panelHeightClass?: string; 
  onAudioSegmentSynthesized: (lineId: string, duration: number) => void;
}

// ç°¡åŒ–çš„ç’°å¢ƒè®Šæ•¸ç²å–å‡½æ•¸
const getEnvVar = (key: string): string | undefined => {
  if (typeof window !== 'undefined') {
    return (window as any)?.env?.[key] || (import.meta as any)?.env?.[`VITE_${key}`] || (import.meta as any)?.env?.[key];
  }
  return process.env[key] || process.env[`VITE_${key}`];
};

const AccordionSection: React.FC<{ title: string, icon?: React.ReactNode, children: React.ReactNode, defaultOpen?: boolean, id?: string }> = ({ title, icon, children, defaultOpen = false, id }) => {
  const [isOpen, setIsOpen] = useState(defaultOpen);
  const sectionId = id || `accordion-${title.replace(/\s+/g, '-').toLowerCase()}`;
  return (
    <div className="border border-slate-700 rounded-lg">
      <button
        onClick={() => setIsOpen(!isOpen)}
        className="w-full flex justify-between items-center p-3 bg-slate-800 hover:bg-slate-750 transition-colors rounded-t-lg"
        aria-expanded={isOpen}
        aria-controls={sectionId}
      >
        <div className="flex items-center">
          {icon && <span className="mr-2">{icon}</span>}
          <span className="font-semibold text-slate-200">{title}</span>
        </div>
        {isOpen ? <ChevronUpIcon className="w-5 h-5 text-slate-400" /> : <ChevronDownIcon className="w-5 h-5 text-slate-400" />}
      </button>
      {isOpen && <div id={sectionId} className="p-4 bg-slate-850 rounded-b-lg space-y-3">{children}</div>}
    </div>
  );
};

export const RightPanel: React.FC<RightPanelProps> = ({
  scriptMode,
  setScriptMode,
  speakers,
  updateSpeaker,
  onGenerateScript,
  onGenerateSeoMeta,
  onDownloadScriptText,
  onDownloadTimedTranscriptSrt,
  seoMeta,
  rssFeedUrl,
  setRssFeedUrl,
  isLoading,
  isSynthesizingAudio,
  setIsSynthesizingAudio,
  setError,
  dialogLines,
  actualAudioDurations,
  panelHeightClass,
  onAudioSegmentSynthesized,
}) => {
  const handleSpeakerChange = (index: number, field: keyof Omit<Speaker, 'id'|'color'|'dotColor'>, value: string) => {
    updateSpeaker(index, { ...speakers[index], [field]: value });
  };

  const synthesizeSpeechInternal = useCallback(async (text: string, voiceId: string, languageCode: string = 'cmn-TW'): Promise<string | null> => {
    const apiKey = process.env.GOOGLE_CLOUD_TTS_API_KEY;
    const projectId = process.env.VERTEX_AI_PROJECT_ID;
    const region = process.env.VERTEX_AI_REGION;

    if (!apiKey) {
      setError("Google Cloud API Key (GOOGLE_CLOUD_TTS_API_KEY) not found. Please configure it in .env.");
      return null;
    }
    if (!projectId || !region) {
      setError("Vertex AI Project ID or Region not found. Please ensure VERTEX_AI_PROJECT_ID and VERTEX_AI_REGION are configured in your .env file and Vite restarted.");
      return null;
    }
    
    const vertexApiEndpoint = `https://${region}-aiplatform.googleapis.com/v1/projects/${projectId}/locations/${region}/publishers/google/models/texttospeech:predict`;

    try {
      const response = await fetch(vertexApiEndpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`, 
        },
        body: JSON.stringify({
          "instances": [ 
            {
              "input": { "text": text },
              "voice": { 
                "languageCode": languageCode, 
                "name": voiceId 
              },
              "audioConfig": { "audioEncoding": "MP3" }
            }
          ]
        }),
      });

      if (!response.ok) {
        const errorData = await response.json();
        console.error("Vertex AI TTS API Error:", errorData);
        throw new Error(errorData.error?.message || `Vertex AI TTS API request failed: ${response.statusText}`);
      }

      const result = await response.json();
      if (result.predictions && result.predictions.length > 0 && result.predictions[0].audioContent) {
        return result.predictions[0].audioContent; 
      } else {
        console.error("Vertex AI TTS API did not return expected audio content structure:", result);
        throw new Error("Vertex AI TTS API did not return audio content in the expected format.");
      }
    } catch (e) {
      console.error("Error synthesizing speech via Vertex AI:", e);
      throw e; 
    }
  }, [setError]);

  // ğŸ­ Gemini TTS å¤šäººå°è©±èªéŸ³åˆæˆ - ä½¿ç”¨ä»£ç†è·¯å¾‘
  const synthesizeMultiSpeakerWithGemini = useCallback(async (dialogLines: DialogLine[], speakers: Speaker[]): Promise<string | null> => {
    const geminiApiKey = getEnvVar('API_KEY');

    if (!geminiApiKey) {
      setError("Gemini API Key (API_KEY) æœªæ‰¾åˆ°ã€‚");
      return null;
    }

    // æ§‹å»ºå°è©±è…³æœ¬
    const script = dialogLines.map(line => {
      const speaker = speakers.find(s => s.id === line.speakerId);
      return `${speaker?.name || 'Speaker'}: ${line.text}`;
    }).join('\n');

    // æ§‹å»ºå¤šäººèªéŸ³é…ç½®ï¼ˆæœ€å¤š2äººï¼‰
    const activeSpeakers = scriptMode === ScriptMode.SINGLE ? [speakers[0]] : speakers.slice(0, 2);
    const speakerConfigs = activeSpeakers.map(speaker => ({
      speaker: speaker.name,
      voiceConfig: {
        prebuiltVoiceConfig: {
          voiceName: speaker.voice
        }
      }
    }));

    const prompt = `TTS the following conversation between ${activeSpeakers.map(s => s.name).join(' and ')}:\n${script}`;

    try {
      // ä½¿ç”¨ä»£ç†è·¯å¾‘
      const response = await fetch('/api/gemini/v1beta/models/gemini-2.5-flash-preview-tts:generateContent', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-goog-api-key': geminiApiKey,
        },
        body: JSON.stringify({
          contents: [{
            parts: [{
              text: prompt
            }]
          }],
          generationConfig: {
            responseModalities: ["AUDIO"],
            speechConfig: {
              multiSpeakerVoiceConfig: {
                speakerVoiceConfigs: speakerConfigs
              }
            }
          }
        }),
      });

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(errorData.error?.message || `Gemini å¤šäººå°è©± TTS å¤±æ•—: ${response.statusText}`);
      }

      const result = await response.json();
      if (result.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data) {
        return result.candidates[0].content.parts[0].inlineData.data;
      } else {
        throw new Error("Gemini TTS æ²’æœ‰è¿”å›éŸ³é »å…§å®¹ã€‚");
      }
    } catch (e) {
      console.error("Gemini å¤šäººå°è©± TTS éŒ¯èª¤:", e);
      throw e;
    }
  }, [setError, scriptMode]);

  // ğŸµ ç´” Gemini TTS èªéŸ³åˆæˆ
  const synthesizeSpeechInternal = useCallback(async (text: string, voiceId: string): Promise<string | null> => {
    return await synthesizeWithGeminiTTS(text, voiceId);
  }, [synthesizeWithGeminiTTS]);

  // ğŸ™ï¸ èªéŸ³é è¦½
  const handlePreviewVoice = useCallback(async (speakerIndex: number) => {
    const speaker = speakers[speakerIndex];
    if (!speaker || isSynthesizingAudio) return;

    setIsSynthesizingAudio(true);
    setError(null);

    const voiceOption = AVAILABLE_VOICES.find(v => v.id === speaker.voice);
    const voiceStyleName = voiceOption ? voiceOption.name : "é è¨­é¢¨æ ¼";
    const speakerName = speaker.name || `ç™¼è¨€äºº ${speakerIndex + 1}`;
    const textToSpeak = `é€™æ˜¯ ${speakerName} ä½¿ç”¨ ${voiceStyleName} é¢¨æ ¼è¨­å®šçš„èªéŸ³é è¦½ã€‚ä½ å¥½å—ï¼Ÿé€™æ˜¯ç”±Vertex AI Text-to-Speechç”¢ç”Ÿã€‚`;
    
    try {
        const audioContentBase64 = await synthesizeSpeechInternal(textToSpeak, speaker.voice);
        if (audioContentBase64) {
            const audioSrc = `data:audio/wav;base64,${audioContentBase64}`;
            const audio = new Audio(audioSrc);
            audio.play().catch(e => {
                console.error("æ’­æ”¾éŸ³é »éŒ¯èª¤:", e);
                setError("ç„¡æ³•æ’­æ”¾é è¦½èªéŸ³ï¼Œç€è¦½å™¨å¯èƒ½é™åˆ¶äº†è‡ªå‹•æ’­æ”¾ã€‚");
            });
        }
    } catch (e) {
        setError(e instanceof Error ? e.message : "èªéŸ³é è¦½åˆæˆæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ã€‚");
    } finally {
        setIsSynthesizingAudio(false);
    }
  }, [speakers, synthesizeSpeechInternal, setError, isSynthesizingAudio, setIsSynthesizingAudio]);

  // ğŸš€ å®Œæ•´ Podcast èªéŸ³ç”Ÿæˆ
  const handleGenerateFullPodcastAudio = useCallback(async () => {
    if (dialogLines.length === 0) {
      setError("è«‹å…ˆç”Ÿæˆè…³æœ¬æ‰èƒ½ç”¢ç”ŸèªéŸ³ã€‚");
      return;
    }

    setIsSynthesizingAudio(true);
    setError(null);

    try {
      // å„ªå…ˆä½¿ç”¨ Gemini å¤šäººå°è©± TTSï¼ˆé©ç”¨æ–¼å¤šäººæ¨¡å¼ä¸”èªªè©±è€… â‰¤ 2äººï¼‰
      if (scriptMode === ScriptMode.MULTI && speakers.length <= 2) {
        console.log("ä½¿ç”¨ Gemini å¤šäººå°è©± TTS ç”Ÿæˆå®Œæ•´å°è©±...");
        
        const audioContentBase64 = await synthesizeMultiSpeakerWithGemini(dialogLines, speakers);
        
        if (audioContentBase64) {
          const byteCharacters = atob(audioContentBase64);
          const byteNumbers = new Array(byteCharacters.length);
          for (let j = 0; j < byteCharacters.length; j++) {
            byteNumbers[j] = byteCharacters.charCodeAt(j);
          }
          const byteArray = new Uint8Array(byteNumbers);
          const blob = new Blob([byteArray], { type: 'audio/wav' });

          // ç›´æ¥ä¸‹è¼‰å®Œæ•´å°è©±
          const link = document.createElement('a');
          link.href = URL.createObjectURL(blob);
          link.download = 'gemini_podcast_full_conversation.wav';
          document.body.appendChild(link);
          link.click();
          document.body.removeChild(link);
          URL.revokeObjectURL(link.href);

          // ç‚ºæ‰€æœ‰è¡Œè¨­ç½®ä¼°ç®—æ™‚é•·
          dialogLines.forEach(line => {
            const lineDuration = line.text.length * 0.1;
            onAudioSegmentSynthesized(line.id, lineDuration);
          });
        }
      } else {
        console.log("ä½¿ç”¨é€è¡ŒéŸ³è¨Šç”Ÿæˆæ¨¡å¼...");
        
        const audioSegments: { name: string; data: Blob }[] = [];
        let hasErrorOccurred = false;

        for (let i = 0; i < dialogLines.length; i++) {
          const line = dialogLines[i];
          const speaker = speakers.find(s => s.id === line.speakerId);

          if (!speaker) {
            setError(`ç¬¬ ${i + 1} è¡Œå°è©±æ‰¾ä¸åˆ°ç™¼è¨€äººè¨­å®šã€‚`);
            hasErrorOccurred = true;
            break; 
          }
          
          try {
            const audioContentBase64 = await synthesizeSpeechInternal(line.text, speaker.voice);

            if (audioContentBase64) {
              const byteCharacters = atob(audioContentBase64);
              const byteNumbers = new Array(byteCharacters.length);
              for (let j = 0; j < byteCharacters.length; j++) {
                byteNumbers[j] = byteCharacters.charCodeAt(j);
              }
              const byteArray = new Uint8Array(byteNumbers);
              const blob = new Blob([byteArray], { type: 'audio/wav' });
              
              // ä¼°ç®—æ™‚é•·
              const lineDuration = line.text.length * 0.1;
              onAudioSegmentSynthesized(line.id, lineDuration);

              const safeSpeakerName = speaker.name.replace(/[^\w\s\u4e00-\u9fa5]/gi, '').replace(/\s+/g, '_'); 
              const fileName = `podcast_segment_${String(i + 1).padStart(2, '0')}_${safeSpeakerName}.wav`;
              audioSegments.push({ name: fileName, data: blob });
            } else {
               setError(`ç¬¬ ${i + 1} è¡ŒèªéŸ³åˆæˆå¤±æ•—ï¼Œæœªæ”¶åˆ°éŸ³è¨Šå…§å®¹ã€‚`);
               hasErrorOccurred = true;
               break;
            }
          } catch (e) {
            console.error(`ç¬¬ ${i + 1} è¡ŒéŸ³é »åˆæˆéŒ¯èª¤:`, e);
            setError(`ç¬¬ ${i + 1} è¡ŒèªéŸ³åˆæˆå¤±æ•—: ${e instanceof Error ? e.message : "æœªçŸ¥éŒ¯èª¤"}`);
            hasErrorOccurred = true;
            break; 
          }
        } 

        if (!hasErrorOccurred && audioSegments.length > 0) {
          try {
            const zip = new JSZip();
            audioSegments.forEach(segment => {
              zip.file(segment.name, segment.data);
            });
            
            const zipBlob = await zip.generateAsync({ type: "blob" });
            const link = document.createElement('a');
            link.href = URL.createObjectURL(zipBlob);
            link.download = 'gemini_podcast_audio_segments.zip';
            document.body.appendChild(link);
            link.click();
            document.body.removeChild(link);
            URL.revokeObjectURL(link.href);
          } catch (e) {
            console.error("å‰µå»ºæˆ–ä¸‹è¼‰ZIPæª”æ¡ˆéŒ¯èª¤:", e);
            setError(`ç”Ÿæˆ ZIP å£“ç¸®æª”å¤±æ•—: ${e instanceof Error ? e.message : "æœªçŸ¥éŒ¯èª¤"}`);
          }
        }
      }
    } catch (e) {
      setError(`èªéŸ³åˆæˆå¤±æ•—: ${e instanceof Error ? e.message : "æœªçŸ¥éŒ¯èª¤"}`);
    } finally {
      setIsSynthesizingAudio(false);
    }
  }, [dialogLines, speakers, scriptMode, synthesizeMultiSpeakerWithGemini, synthesizeSpeechInternal, setError, setIsSynthesizingAudio, onAudioSegmentSynthesized]);

  const canDownloadTimedTranscript = dialogLines.length > 0 && 
                                   Object.keys(actualAudioDurations).length === dialogLines.length &&
                                   dialogLines.every(line => actualAudioDurations[line.id] !== undefined && actualAudioDurations[line.id] >= 0);

  return (
    <div 
      className={`lg:w-1/3 bg-slate-900 p-6 rounded-lg shadow-xl space-y-6 overflow-y-auto custom-scrollbar 
                 lg:sticky lg:top-4 ${panelHeightClass || ''}`}
    >
      <h2 className="text-lg font-semibold text-emerald-400 mb-1">4. åŸ·è¡Œèˆ‡è¨­å®š (Run & Settings)</h2>
      
      <div className="space-y-3">
        <label htmlFor="ai-model-display" className="block text-sm font-medium text-slate-300">AI æ–‡å­—æ¨¡å‹ (AI Text Model)</label>
        <div id="ai-model-display" className="bg-slate-800 p-3 rounded-md text-sm border border-slate-700">{GEMINI_MODEL_TEXT} (Text Generation)</div>
      
        <fieldset className="space-y-1">
            <legend className="block text-sm font-medium text-slate-300 mb-1">æ¨¡å¼ (Mode)</legend>
            <div className="grid grid-cols-2 gap-2">
              <Button onClick={() => setScriptMode(ScriptMode.SINGLE)} variant={scriptMode === ScriptMode.SINGLE ? 'primary' : 'secondary'} fullWidth aria-pressed={scriptMode === ScriptMode.SINGLE}>
                <UserIcon className="w-5 h-5 mr-2"/> å–®äººä¸»æŒ
              </Button>
              <Button onClick={() => setScriptMode(ScriptMode.MULTI)} variant={scriptMode === ScriptMode.MULTI ? 'primary' : 'secondary'} fullWidth aria-pressed={scriptMode === ScriptMode.MULTI}>
                <UsersIcon className="w-5 h-5 mr-2"/> å¤šäººå°è©±
              </Button>
            </div>
        </fieldset>
      </div>

      <AccordionSection title="èªéŸ³è¨­å®š (Voice Settings)" icon={<Cog8ToothIcon className="w-5 h-5 text-slate-400" />} defaultOpen={true} id="voice-settings-section">
        {/* Gemini TTS ç‹€æ…‹æŒ‡ç¤ºå™¨ */}
        <div className="mb-4 p-3 bg-slate-800 rounded-md border border-emerald-600/50">
          <div className="flex items-center">
            <span className="w-2 h-2 rounded-full bg-emerald-400 mr-2"></span>
            <span className="text-emerald-400 font-semibold text-sm">ğŸš€ Gemini AI åŸç”Ÿ TTS å·²å•Ÿç”¨</span>
          </div>
          <p className="text-xs text-slate-400 mt-1">âœ¨ 30ç¨®é«˜å“è³ªèªéŸ³ | ğŸ­ æ™ºèƒ½å¤šäººå°è©± | ğŸŒ 24ç¨®èªè¨€æ”¯æ´</p>
        </div>

        {(scriptMode === ScriptMode.SINGLE ? [speakers[0]] : speakers).map((speaker, originalIndex) => {
          const speakerArrayIndex = scriptMode === ScriptMode.SINGLE ? speakers.findIndex(s => s.id === speaker.id) : originalIndex;
          if (!speaker) return null; 

          return (
          <div key={speaker.id} className="p-3 border border-slate-700 rounded-md bg-slate-800">
            <div className="flex items-center mb-2">
              <span className={`w-3 h-3 rounded-full ${speaker.dotColor} mr-2`}></span>
              <h4 className="font-semibold text-sm text-slate-300">ç™¼è¨€äºº {originalIndex + 1} è¨­å®š</h4>
            </div>
            <TextInput
              label="åç¨± (Name)"
              id={`speaker-name-${speaker.id}`}
              value={speaker.name}
              onChange={(e) => handleSpeakerChange(speakerArrayIndex, 'name', e.target.value)}
              placeholder={`ä¾‹å¦‚ï¼šä¸»æŒäºº ${originalIndex + 1}`}
            />
            <div className="flex items-end space-x-2">
                <Select
                    label="Vertex AI / Google Cloud TTS èªéŸ³"
                    id={`speaker-voice-style-${speaker.id}`}
                    value={speaker.voice} 
                    onChange={(e) => handleSpeakerChange(speakerArrayIndex, 'voice', e.target.value)}
                    options={AVAILABLE_VOICES.map(v => ({ value: v.id, label: v.name }))}
                    className="flex-grow"
                    helperText="é¸æ“‡ Vertex AI / Google Cloud TTS èªéŸ³ã€‚é¢¨æ ¼æè¿°ç”¨æ–¼AIè…³æœ¬ç”Ÿæˆï¼Œæ­¤è™•é¸æ“‡å¯¦éš›ç™¼è²èªéŸ³ã€‚"
                />
                <Button 
                    onClick={() => handlePreviewVoice(speakerArrayIndex)} 
                    variant="outline" 
                    size="md" 
                    className="mb-3 flex-shrink-0"
                    aria-label={`é è¦½ ${speaker.name} çš„ Vertex AI TTS èªéŸ³`}
                    disabled={isSynthesizingAudio}
                >
                    {isSynthesizingAudio && 
                     speakers[speakerArrayIndex] === speaker && <Spinner/>} 
                    {(!isSynthesizingAudio || speakers[speakerArrayIndex] !== speaker) && <PlayCircleIcon className="w-5 h-5"/>}
                </Button>
            </div>
          </div>
        )}))}
      </AccordionSection>
      
      <div className="space-y-3 pt-3">
        <Button onClick={onGenerateScript} disabled={isLoading || isSynthesizingAudio} fullWidth variant="primary" aria-label="Generate podcast script">
          {(isLoading && !isSynthesizingAudio) ? <Spinner /> : <SparklesIcon className="w-5 h-5 mr-2" />} ç”Ÿæˆè…³æœ¬
        </Button>
         <Button onClick={onGenerateSeoMeta} disabled={isLoading || isSynthesizingAudio || dialogLines.length === 0} fullWidth variant="secondary" aria-label="Generate SEO metadata">
           {(isLoading && !isSynthesizingAudio) ? <Spinner /> : <SparklesIcon className="w-5 h-5 mr-2" />} ç”Ÿæˆ SEO Meta
        </Button>
      </div>

      <AccordionSection title="ç”¢å‡ºèˆ‡ç™¼ä½ˆ (Output & Publishing)" icon={<DocumentTextIcon className="w-5 h-5 text-slate-400" />} id="output-publishing-section">
        {seoMeta && (
          <div className="bg-slate-800 p-3 rounded-md border border-slate-700">
            <h4 className="font-semibold text-sm text-sky-400 mb-1">SEO Meta:</h4>
            <p className="text-xs text-slate-300"><strong>Title:</strong> {seoMeta.title}</p>
            <p className="text-xs text-slate-300"><strong>Description:</strong> {seoMeta.description}</p>
          </div>
        )}
        <Button onClick={onDownloadScriptText} fullWidth variant="outline" aria-label="ä¸‹è¼‰è…³æœ¬æ–‡å­— (TXT)" disabled={dialogLines.length === 0 || isSynthesizingAudio}>
          <DocumentTextIcon className="w-5 h-5 mr-2"/> ä¸‹è¼‰è…³æœ¬æ–‡å­— (TXT)
        </Button>
        <Button 
          onClick={onDownloadTimedTranscriptSrt} 
          fullWidth 
          variant="outline" 
          aria-label="ä¸‹è¼‰æ™‚é–“ç¢¼é€å­—ç¨¿ (SRT)" 
          disabled={!canDownloadTimedTranscript || isSynthesizingAudio}
          title={!canDownloadTimedTranscript ? "è«‹å…ˆã€Œç”¢ç”Ÿ AI Podcast èªéŸ³ã€ä»¥ç²å–ç²¾ç¢ºæ™‚é–“æˆ³ã€‚" : "ä¸‹è¼‰ SRT æ ¼å¼çš„æ™‚é–“ç¢¼é€å­—ç¨¿"}
        >
          <ClockIcon className="w-5 h-5 mr-2"/> ä¸‹è¼‰æ™‚é–“ç¢¼é€å­—ç¨¿ (SRT)
        </Button>
        <Button 
          onClick={handleGenerateFullPodcastAudio} 
          fullWidth 
          variant="outline" 
          aria-label="ç”¢ç”Ÿ AI Podcast èªéŸ³" 
          disabled={dialogLines.length === 0 || isSynthesizingAudio}
        >
         {isSynthesizingAudio ? <Spinner/> : <MusicalNoteIcon className="w-5 h-5 mr-2"/>} 
         ğŸ­ ç”¢ç”Ÿ AI Podcast èªéŸ³ (æ™ºèƒ½å¤šäººå°è©±)
        </Button>
        <TextInput
          label="RSS Feed ç™¼ä½ˆç¶²å€ (RSS Feed URL - for reference)"
          id="rss-feed-url"
          value={rssFeedUrl}
          onChange={(e) => setRssFeedUrl(e.target.value)}
          placeholder="https://anchor.fm/s/your-id/podcast/rss"
          helperText="æ­¤æ¬„ä½åƒ…ä¾›è¨˜éŒ„ï¼Œç›®å‰ä¸æ”¯æ´è‡ªå‹•ä¸Šå‚³ã€‚"
        />
      </AccordionSection>
    </div>
  );
};